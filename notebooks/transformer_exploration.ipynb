{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishantbhansali/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import hiq\n",
    "from llama import ModelArgs, Tokenizer, LLaMA # Transformer\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import hiq\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 1\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1 # 32 // 1 = 32\n",
    "        self.head_dim = args.dim // args.n_heads # 4096 // 32 = 128\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )  # (4096, 4096)\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "            # (1,1024,32,128)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "            # (1,1024,32,128)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # (1,8,4096)\n",
    "        start_pos: int, # 0 (initially)\n",
    "        freqs_cis: torch.Tensor,  # (1024, 64)\n",
    "        mask: Optional[torch.Tensor],  # (1,1,8,8)\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # all of shape (1,8,4096)\n",
    "        print(f\" shape of xq is {xq.shape}\")\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "        print(f\" shape of xq is {xq.shape}\")\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # (1,8,32,128), (1,8,32,128)\n",
    "        print(f\" shape of xq is {xq.shape}\")\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq) # (1,1024,32,128)\n",
    "        self.cache_v = self.cache_v.to(xq) # (1,1024,32,128)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk # (1,1024,32,128)\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv # (1,1024,32,128)\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen] # (1,1024,32,128)\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen] # (1,1024,32,128)\n",
    "\n",
    "        xq = xq.transpose(1, 2) # (1,32,8,128)\n",
    "        keys = keys.transpose(1, 2) # (1,32,1024,128)\n",
    "        values = values.transpose(1, 2) # (1,32,1024,128)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) # (1,32,8,1024)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq) # (1,32,8,1024)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim) # (1,32,8,128)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # (1,8,4096)\n",
    "\n",
    "        return self.wo(output) # (1,8,4096)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int, # 4096\n",
    "        hidden_dim: int, # 4 * 4096 = 16384\n",
    "        multiple_of: int, # 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3) # 2 * 16384 / 3 = 10922\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) # 256 * (10922 + 256 - 1) // 256 = 11177\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False) # (11177, 4096)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (1,8,4096)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "            # 4096, 4 * 4096, 256\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # (1,8,4096)\n",
    "        start_pos: int, # 0 (initially)\n",
    "        freqs_cis: torch.Tensor, # (1024, 64)\n",
    "        mask: Optional[torch.Tensor], # (1,1,8,8)\n",
    "    ):\n",
    "        # this is a skip connection\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "            # (1,8,4096), 0, (1024, 64), (1,1,8,8)\n",
    "        ) # (1,8,4096)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out # (1,8,4096)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params # ModelArgs\n",
    "        self.vocab_size = params.vocab_size # 32_000\n",
    "        self.n_layers = params.n_layers # 32\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim) # (32_000, 4096)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps) # shape of output is same as input\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False) # (4096, 32_000)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "            # 4096 // 32 = 128, 1024 * 2\n",
    "        ) # torch.Size([2048, 64])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape # (1,8)\n",
    "        h = self.tok_embeddings(tokens) # (1,8,4096)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] # torch.Size([8, 64])\n",
    "        print(f\"shape of freqs_cis: {freqs_cis.shape}\")\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            ) # (1,1,8,8) , filled with -inf\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "            # (1,1,8,8) , filled with -inf, but only the upper triangle, lower triangle is 0\n",
    "            # diagnol = start_pos + 1, so the first 8 tokens are not masked, it basically pushes the diagonola above\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h) # (1,8,4096)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits # (1, 4096) * (4096, 32_000) = (1, 32_000)\n",
    "        return output.float() # (1, 32_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "# create a model args object\n",
    "# ModelArgs is a a simple dataclass that contains the parameters for the model\n",
    "# file in llama/model_single.py\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    ")\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of freqs_cis: torch.Size([8, 64])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 4096])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n",
      " shape of xq is torch.Size([1, 8, 32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randint(0, 32_000, (1, 8)).cuda(), 0).shape # (1, 32_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 1\n",
    "tokenizer_path = '/home/nishantbhansali/MyProject/model/tokenizer.model'\n",
    "ckpt_dir = '/home/nishantbhansali/MyProject/model/7B'\n",
    "tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "print(tokenizer.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, -1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_id,tokenizer.eos_id,tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "ckpt_path = checkpoints[0]\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "# create a model args object\n",
    "# ModelArgs is a a simple dataclass that contains the parameters for the model\n",
    "# file in llama/model_single.py\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    ")\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['layers.0.attention.inner_attention.rope.freqs', 'layers.1.attention.inner_attention.rope.freqs', 'layers.2.attention.inner_attention.rope.freqs', 'layers.3.attention.inner_attention.rope.freqs', 'layers.4.attention.inner_attention.rope.freqs', 'layers.5.attention.inner_attention.rope.freqs', 'layers.6.attention.inner_attention.rope.freqs', 'layers.7.attention.inner_attention.rope.freqs', 'layers.8.attention.inner_attention.rope.freqs', 'layers.9.attention.inner_attention.rope.freqs', 'layers.10.attention.inner_attention.rope.freqs', 'layers.11.attention.inner_attention.rope.freqs', 'layers.12.attention.inner_attention.rope.freqs', 'layers.13.attention.inner_attention.rope.freqs', 'layers.14.attention.inner_attention.rope.freqs', 'layers.15.attention.inner_attention.rope.freqs', 'layers.16.attention.inner_attention.rope.freqs', 'layers.17.attention.inner_attention.rope.freqs', 'layers.18.attention.inner_attention.rope.freqs', 'layers.19.attention.inner_attention.rope.freqs', 'layers.20.attention.inner_attention.rope.freqs', 'layers.21.attention.inner_attention.rope.freqs', 'layers.22.attention.inner_attention.rope.freqs', 'layers.23.attention.inner_attention.rope.freqs', 'layers.24.attention.inner_attention.rope.freqs', 'layers.25.attention.inner_attention.rope.freqs', 'layers.26.attention.inner_attention.rope.freqs', 'layers.27.attention.inner_attention.rope.freqs', 'layers.28.attention.inner_attention.rope.freqs', 'layers.29.attention.inner_attention.rope.freqs', 'layers.30.attention.inner_attention.rope.freqs', 'layers.31.attention.inner_attention.rope.freqs'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=32000, multiple_of=256, norm_eps=1e-06, max_batch_size=1, max_seq_len=1024)\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of parameters in model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",  # removed: keep only one prompt\n",
    "    ]\n",
    "\n",
    "max_gen_len=256\n",
    "temperature=0.8\n",
    "top_p=0.95\n",
    "max_seq_len= 1024\n",
    "max_batch_size= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bsz \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(prompts) \u001b[39m# 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparams \u001b[39m# those same ModelArgs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m bsz \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mmax_batch_size, (bsz, params\u001b[39m.\u001b[39mmax_batch_size)\n\u001b[1;32m      5\u001b[0m prompt_tokens \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(x, bos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, eos\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m prompts]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "bsz = len(prompts) # 1\n",
    "params = model.params # those same ModelArgs\n",
    "assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "# [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n",
    "\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens]) # 8\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens]) # 8\n",
    "\n",
    "total_len = min(params.max_seq_len, max_gen_len + max_prompt_size) # 264\n",
    "\n",
    "tokens = torch.full((bsz, total_len), tokenizer.pad_id).cuda().long()\n",
    "# a tensor of size (1, 264) filled with -1's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).cuda().long()\n",
    "input_text_mask = tokens != tokenizer.pad_id # a tensor of size (1, 264) filled with True's ,\n",
    "# where tokens is not -1, other wise False\n",
    "start_pos = min_prompt_size # 8\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.8516, -14.3594,   3.5098,  ...,  -6.0430,  -7.9609,  -3.8164]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = tokens[:, prev_pos:8]\n",
    "logits = model(i, prev_pos)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.2456e-14, 8.4374e-17, 4.2344e-07,  ..., 2.7601e-12, 2.5102e-13,\n",
       "         4.4632e-11]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8471]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = sample_top_p(torch.softmax(logits / temperature, dim=-1), top_p)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8471]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.where(\n",
    "        input_text_mask[:, 8], tokens[:, 8], next_token\n",
    "    )\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:, 8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8471]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokens[:, 8] = next_token\n",
    "i = tokens[:, 8:9]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(i, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_pos in range(start_pos, total_len):\n",
    "    i = tokens[:, prev_pos:cur_pos]\n",
    "    logits = model(i, prev_pos) # torch.Size([1, 32000])\n",
    "\n",
    "    if temperature > 0:\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = sample_top_p(probs, top_p)\n",
    "    else:\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "    next_token = next_token.reshape(-1)\n",
    "    # only replace token if prompt has already been generated\n",
    "    next_token = torch.where(\n",
    "        input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "    )\n",
    "    tokens[:, cur_pos] = next_token\n",
    "    prev_pos = cur_pos\n",
    "    \n",
    "    # if self._should_stop(tokens, prompt_tokens, stop_ids, stop_words):\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(output_text, stop_words=None, threshold=10):\n",
    "    sentences = output_text.split(\".\")\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) > threshold and sentence[-1] == \".\":\n",
    "            filtered_sentences.append(sentence)\n",
    "    r = '.'.join(sentences).strip()\n",
    "    if stop_words:\n",
    "        for w in stop_words:\n",
    "            if r.endswith(w):\n",
    "                r = r[0:-len(w)].strip()\n",
    "    if r[-1] != '.':\n",
    "        r += '...'\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(278, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meaning'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0][4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I believe the meaning of life is to do whatever you can to make the world a better place for all, and to create memories.\\nIt\\'s easy to become depressed about the direction of the world these days, but if you focus on the good and the positive things going on, you\\'ll find you have a more optimistic view of the world. If you can bring the light into the world, you can make a difference.\\nThat\\'s what my series of streetscapes does - it brings the light into the world.\\nThe easiest way to make the world a better place is to smile at everyone you meet.\\nI have a friend who was in a meeting with a large group of managers at a company where I used to work, and after he left, one of the managers said to him, \"I\\'ve never seen you smile before. What\\'s up?\" He said, \"I didn\\'t think my smile would make a difference.\"\\nI think my smile makes a difference.\\nI think the life of a street photographer is the ideal job for someone who wants to make the world a better place. When you go out on the street and you make people smile, you make people happy, you bring the light into the world.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[tokens == tokenizer.pad_id] = tokenizer.eos_id\n",
    "decoded = []\n",
    "for i, t in enumerate(tokens.tolist()):\n",
    "    # cut to max gen len\n",
    "    t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
    "    # cut to eos tok if any\n",
    "    try:\n",
    "        t = t[: t.index(tokenizer.eos_id)]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    decoded.append(tokenizer.decode(t))\n",
    "#print(decoded)\n",
    "[postprocessing(i, None) for i in decoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precompute_freqs_cis(128,2048).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    print(\"sdfsd\",(x.shape[1], x.shape[-1]))\n",
    "    print(\"dfsdf\",freqs_cis.shape)\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] # [1,8,1,64]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor, # (1,8,32,128)\n",
    "    xk: torch.Tensor, # (1,8,32,128)\n",
    "    freqs_cis: torch.Tensor, # (8,64)\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)) # (1,8,32,128) -> (1,8,32,64,2)\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) # (1,8,32,128) -> (1,8,32,64,2)\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_) # (1,8,1,64)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3) # (1,8,32,64,2) -> (1,8,32,128) \n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3) # (1,8,32,64,2) -> (1,8,32,128)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk) # (1,8,32,128), (1,8,32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32, 64, 2])\n",
      "torch.Size([1, 8, 32, 64])\n",
      "torch.Size([1, 8, 32, 64])\n",
      "torch.Size([8, 64])\n",
      "sdfsd (8, 32)\n",
      "dfsdf torch.Size([8, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(freqs_cis\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m \u001b[39m# freqs_cis = reshape_for_broadcast(freqs_cis, xq)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(freqs_cis.shape)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m apply_rotary_emb(xq,xk,freqs_cis)\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m     15\u001b[0m xq_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_complex(xq\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mxq\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m     16\u001b[0m xk_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_complex(xk\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mxk\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m freqs_cis \u001b[39m=\u001b[39m reshape_for_broadcast(freqs_cis, xq_)\n\u001b[1;32m     18\u001b[0m xq_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(xq_ \u001b[39m*\u001b[39m freqs_cis)\u001b[39m.\u001b[39mflatten(\u001b[39m3\u001b[39m)\n\u001b[1;32m     19\u001b[0m xk_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(xk_ \u001b[39m*\u001b[39m freqs_cis)\u001b[39m.\u001b[39mflatten(\u001b[39m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m, in \u001b[0;36mreshape_for_broadcast\u001b[0;34m(freqs_cis, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msdfsd\u001b[39m\u001b[39m\"\u001b[39m,(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdfsdf\u001b[39m\u001b[39m\"\u001b[39m,freqs_cis\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m \u001b[39massert\u001b[39;00m freqs_cis\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      7\u001b[0m shape \u001b[39m=\u001b[39m [d \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m ndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x\u001b[39m.\u001b[39mshape)]\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m freqs_cis\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xq = torch.randn(1,8,32,128)\n",
    "xq = xq.reshape(*xq.shape[:-1], -1, 2)\n",
    "print(xq.shape)\n",
    "xq = torch.view_as_complex(xq.float())\n",
    "print(xq.shape)\n",
    "xk = torch.randn(1,8,32,128)\n",
    "xk = xk.reshape(*xk.shape[:-1], -1, 2)\n",
    "xk = torch.view_as_complex(xk.float())\n",
    "print(xk.shape)\n",
    "freqs_cis = precompute_freqs_cis(128,2048)\n",
    "freqs_cis = freqs_cis[:8]\n",
    "print(freqs_cis.shape)\n",
    "# freqs_cis = reshape_for_broadcast(freqs_cis, xq)\n",
    "# print(freqs_cis.shape)\n",
    "apply_rotary_emb(xq,xk,freqs_cis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
