{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishantbhansali/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import hiq\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = '/home/nishantbhansali/MyProject/model/tokenizer.model'\n",
    "ckpt_dir = '/home/nishantbhansali/MyProject/model/7B'\n",
    "tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "print(tokenizer.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "ckpt_path = checkpoints[0]\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "# create a model args object\n",
    "# ModelArgs is a a simple dataclass that contains the parameters for the model\n",
    "# file in llama/model_single.py\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['layers.0.attention.inner_attention.rope.freqs', 'layers.1.attention.inner_attention.rope.freqs', 'layers.2.attention.inner_attention.rope.freqs', 'layers.3.attention.inner_attention.rope.freqs', 'layers.4.attention.inner_attention.rope.freqs', 'layers.5.attention.inner_attention.rope.freqs', 'layers.6.attention.inner_attention.rope.freqs', 'layers.7.attention.inner_attention.rope.freqs', 'layers.8.attention.inner_attention.rope.freqs', 'layers.9.attention.inner_attention.rope.freqs', 'layers.10.attention.inner_attention.rope.freqs', 'layers.11.attention.inner_attention.rope.freqs', 'layers.12.attention.inner_attention.rope.freqs', 'layers.13.attention.inner_attention.rope.freqs', 'layers.14.attention.inner_attention.rope.freqs', 'layers.15.attention.inner_attention.rope.freqs', 'layers.16.attention.inner_attention.rope.freqs', 'layers.17.attention.inner_attention.rope.freqs', 'layers.18.attention.inner_attention.rope.freqs', 'layers.19.attention.inner_attention.rope.freqs', 'layers.20.attention.inner_attention.rope.freqs', 'layers.21.attention.inner_attention.rope.freqs', 'layers.22.attention.inner_attention.rope.freqs', 'layers.23.attention.inner_attention.rope.freqs', 'layers.24.attention.inner_attention.rope.freqs', 'layers.25.attention.inner_attention.rope.freqs', 'layers.26.attention.inner_attention.rope.freqs', 'layers.27.attention.inner_attention.rope.freqs', 'layers.28.attention.inner_attention.rope.freqs', 'layers.29.attention.inner_attention.rope.freqs', 'layers.30.attention.inner_attention.rope.freqs', 'layers.31.attention.inner_attention.rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",  # removed: keep only one prompt\n",
    "    ]\n",
    "\n",
    "max_gen_len=256\n",
    "temperature=0.8\n",
    "top_p=0.95\n",
    "max_seq_len= 1024\n",
    "max_batch_size= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = len(prompts) # 1\n",
    "params = model.params # those same ModelArgs\n",
    "assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "# [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n",
    "\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens]) # 8\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens]) # 8\n",
    "\n",
    "total_len = min(params.max_seq_len, max_gen_len + max_prompt_size) # 264\n",
    "\n",
    "tokens = torch.full((bsz, total_len), tokenizer.pad_id).cuda().long()\n",
    "# a tensor of size (1, 264) filled with -1's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).cuda().long()\n",
    "input_text_mask = tokens != tokenizer.pad_id # a tensor of size (1, 264) filled with True's ,\n",
    "# where tokens is not -1, other wise False\n",
    "start_pos = min_prompt_size # 8\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 32000])\n"
     ]
    }
   ],
   "source": [
    "i = tokens[:, prev_pos:8]\n",
    "print(i.shape)\n",
    "logits = model(i, prev_pos)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4096])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_embeddings = nn.Embedding(model.params.vocab_size, model.params.dim)\n",
    "tok_embeddings(torch.tensor([[1,2,3,4,5,6,7,8]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precompute_freqs_cis(128,2048)[0:0+1024].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim) # (32_000, 4096)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False) # (4096, 32_000)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "            # 4096 // 32 = 128, 1024 * 2\n",
    "        ) # torch.Size([2048, 64])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape # (1,8)\n",
    "        h = self.tok_embeddings(tokens) # (1,8,4096)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] # torch.Size([1024, 64])\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            ) # (1,1,8,8) , filled with -inf\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "            # (1,1,8,8) , filled with -inf, but only the upper triangle, lower triangle is 0\n",
    "            # diagnol = start_pos + 1, so the first 8 tokens are not masked, it basically pushes the diagonola above\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h) # (1,8,4096)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits # (1, 4096) * (4096, 32_000) = (1, 32_000)\n",
    "        return output.float() # (1, 32_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(\n",
    "                (1, 1, 8, 8), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "mask = torch.triu(mask, diagonal=0 + 1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "            # 4096, 4 * 4096, 256\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # (1,8,4096)\n",
    "        start_pos: int, # 0 (initially)\n",
    "        freqs_cis: torch.Tensor, # (1024, 64)\n",
    "        mask: Optional[torch.Tensor], # (1,1,8,8)\n",
    "    ):\n",
    "        # this is a skip connection\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_cis, mask\n",
    "            # (1,8,4096), 0, (1024, 64), (1,1,8,8)\n",
    "        ) # (1,8,4096)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out # (1,8,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11177"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256 * (10922 + 256 - 1) // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10922.666666666666"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 16384 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int, # 4096\n",
    "        hidden_dim: int, # 4 * 4096 = 16384\n",
    "        multiple_of: int, # 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3) # 2 * 16384 / 3 = 10922\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) # 256 * (10922 + 256 - 1) // 256 = 11177\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False) # (11177, 4096)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False) # (4096, 11177)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (1,8,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1 # 32 // 1 = 32\n",
    "        self.head_dim = args.dim // args.n_heads # 4096 // 32 = 128\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )  # (4096, 4096)\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        ) # (4096, 4096)\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "            # (1,1024,32,128)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "            # (1,1024,32,128)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # (1,8,4096)\n",
    "        start_pos: int, # 0 (initially)\n",
    "        freqs_cis: torch.Tensor,  # (1024, 64)\n",
    "        mask: Optional[torch.Tensor],  # (1,1,8,8)\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # all of shape (1,8,4096)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim) # (1,8,32,128)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # (1,8,32,128), (1,8,32,128)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq) # (1,1024,32,128)\n",
    "        self.cache_v = self.cache_v.to(xq) # (1,1024,32,128)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk # (1,1024,32,128)\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv # (1,1024,32,128)\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen] # (1,1024,32,128)\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen] # (1,1024,32,128)\n",
    "\n",
    "        xq = xq.transpose(1, 2) # (1,32,8,128)\n",
    "        keys = keys.transpose(1, 2) # (1,32,1024,128)\n",
    "        values = values.transpose(1, 2) # (1,32,1024,128)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) # (1,32,8,1024)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq) # (1,32,8,1024)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim) # (1,32,8,128)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # (1,8,4096)\n",
    "\n",
    "        return self.wo(output) # (1,8,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m freqs_ciss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1024\u001b[39m,\u001b[39m64\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(freqs_ciss\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m xq, xk \u001b[39m=\u001b[39m apply_rotary_emb(xq, xk, freqs_cis\u001b[39m=\u001b[39;49mfreqs_ciss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(xq\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(xk\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m     22\u001b[0m xq_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_complex(xq\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mxq\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m     23\u001b[0m xk_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_complex(xk\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mxk\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m freqs_cis \u001b[39m=\u001b[39m reshape_for_broadcast(freqs_cis, xq_)\n\u001b[1;32m     25\u001b[0m xq_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(xq_ \u001b[39m*\u001b[39m freqs_cis)\u001b[39m.\u001b[39mflatten(\u001b[39m3\u001b[39m)\n\u001b[1;32m     26\u001b[0m xk_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(xk_ \u001b[39m*\u001b[39m freqs_cis)\u001b[39m.\u001b[39mflatten(\u001b[39m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mreshape_for_broadcast\u001b[0;34m(freqs_cis, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mndim\n\u001b[1;32m     11\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ndim\n\u001b[0;32m---> 12\u001b[0m \u001b[39massert\u001b[39;00m freqs_cis\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m shape \u001b[39m=\u001b[39m [d \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m ndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x\u001b[39m.\u001b[39mshape)]\n\u001b[1;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m freqs_cis\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xq = torch.randn(1,8,32,128)\n",
    "xk = torch.randn(1,8,32,128)\n",
    "freqs_ciss = torch.randn(1024,64)\n",
    "print(freqs_ciss.shape)\n",
    "xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_ciss)\n",
    "print(xq.shape)\n",
    "print(xk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = torch.randn(1,8,32,128)\n",
    "xk = torch.randn(1,8,32,128)\n",
    "xv = torch.randn(1,8,32,128)\n",
    "cache_k = torch.zeros(1,1024,32,128)\n",
    "cache_v = torch.zeros(1,1024,32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "cache_k = cache_k.to(xq) # (1,1024,32,128)\n",
    "cache_v = cache_v.to(xq) # (1,1024,32,128)\n",
    "print(cache_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_k[:1, 0 : 0 + 8] = xk # (1,1024,32,128)\n",
    "cache_v[:1, 0 : 0 + 8] = xv # (1,1024,32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7062, -0.0164,  1.0580,  ..., -0.3052, -0.6578,  0.2834],\n",
       "          [ 1.2989,  0.5767,  0.6553,  ..., -0.6626, -0.6152,  0.1348],\n",
       "          [-1.8242, -0.2470, -1.0356,  ..., -0.3468, -0.0629, -1.7530],\n",
       "          ...,\n",
       "          [ 0.4131, -1.2540, -0.3932,  ...,  0.5691, -0.5716,  0.5818],\n",
       "          [ 0.1771, -0.1443,  1.4504,  ...,  0.3732, -1.1157,  1.1206],\n",
       "          [-0.3930, -1.0321,  0.4053,  ..., -0.1372,  1.8330, -0.4297]],\n",
       "\n",
       "         [[-1.3268,  0.4006,  0.0696,  ..., -1.0020, -2.3518, -1.3615],\n",
       "          [-0.1848, -0.0380,  0.4700,  ...,  0.9859,  0.9182,  0.7895],\n",
       "          [ 2.6625, -0.2266,  1.1674,  ..., -0.3145,  0.0824,  1.5076],\n",
       "          ...,\n",
       "          [ 0.0749,  1.8606, -0.5492,  ...,  0.4825, -1.1236,  0.3204],\n",
       "          [-1.4965,  0.3555,  0.2601,  ..., -0.4831, -0.3213, -0.5264],\n",
       "          [-0.3599,  1.1146, -1.0524,  ...,  0.1825,  1.3348,  0.9725]],\n",
       "\n",
       "         [[ 0.6107, -0.5555,  0.4862,  ...,  1.3714, -0.5348,  0.8950],\n",
       "          [-0.0682,  2.0073,  0.4322,  ..., -0.5250, -1.2298,  0.1764],\n",
       "          [ 1.6428,  0.7582, -0.2282,  ..., -0.8764,  0.9171,  0.5910],\n",
       "          ...,\n",
       "          [-1.3295, -0.1775,  1.6423,  ...,  1.4107,  0.9089, -0.5831],\n",
       "          [-1.8206,  0.8116,  0.1048,  ...,  0.9659, -0.4838, -1.0666],\n",
       "          [-0.5969,  0.4457,  0.3625,  ...,  0.3640, -0.1302,  0.5903]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32, 128]) torch.Size([1, 8, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "keys = cache_k[:1, : 0 + 8] # (1,8,32,128)\n",
    "values = cache_v[:1, : 0 + 8] # (1,8,32,128)\n",
    "print(keys.shape,values.shape)\n",
    "xq = xq.transpose(1, 2) # (1,32,8,128)\n",
    "keys = keys.transpose(1, 2) # (1,32,8,128)\n",
    "values = values.transpose(1, 2) # (1,32,8,128)\n",
    "xq.shape,keys.shape,values.shape # (1,32,8,128), (1,32,8,128), (1,32,8,128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(128) # matrix multiply of (1,32,8,128) and (1,32,128,8) resulting in # (1,32,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(1,1,8,8)\n",
    "scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
